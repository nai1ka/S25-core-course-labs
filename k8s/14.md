# Lab 14: Kubernetes StatefulSet

## Task 1: Implement StatefulSet in Helm Chart

```bash
❱❱❱ helm install python-time-service --dry-run --debug ./python-time-service
install.go:225: 2025-03-14 23:31:28.9608 +0300 MSK m=+0.046424335 [debug] Original chart version: ""
install.go:242: 2025-03-14 23:31:28.961322 +0300 MSK m=+0.046946168 [debug] CHART PATH: /Users/nai1ka/Projects/S25-core-course-labs/k8s/python-time-service

NAME: python-time-service
LAST DEPLOYED: Fri Mar 14 23:31:29 2025
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
environment:
  author: nai1ka
  course: devops
  debug: false
fullnameOverride: ""
image:
  pullPolicy: Always
  repository: nai1ka/time-service
  tag: latest
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
library-chart:
  global: {}
livenessProbe:
  httpGet:
    path: /
    port: http
nameOverride: ""
nodeSelector: {}
persistence:
  name: python-data
  size: 1Gi
podAnnotations:
  name: ""
podLabels: {}
podManagementPolicy: Parallel
podSecurityContext: {}
readinessProbe:
  httpGet:
    path: /
    port: http
replicaCount: 4
resources:
  limits:
    cpu: 500m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 128Mi
securityContext: {}
service:
  port: 8000
  type: ClusterIP
serviceAccount:
  annotations: null
  automount: true
  create: true
tolerations: []
volumeMounts:
- mountPath: /app/data
  name: python-data

HOOKS:
---
# Source: python-time-service/templates/post-install-hook.yaml
apiVersion: v1
kind: Pod
metadata:
   name: python-time-service-post-install
   annotations:
       "helm.sh/hook": "post-install"
       "helm.sh/hook-delete-policy": "hook-succeeded"
spec:
  containers:
  - name: post-install-container
    image: busybox
    imagePullPolicy: Always
    command: ['sh', '-c', 'echo The post-install hook is running && sleep 10' ]
  restartPolicy: Never
  terminationGracePeriodSeconds: 0
---
# Source: python-time-service/templates/pre-install-hook.yaml
apiVersion: v1
kind: Pod
metadata:
   name: python-time-service-pre-install
   annotations:
       "helm.sh/hook": "pre-install"
       "helm.sh/hook-delete-policy": hook-succeeded
spec:
  containers:
  - name: pre-install-container
    image: busybox
    imagePullPolicy: IfNotPresent
    command: ['sh', '-c', 'echo The pre-install hook is running && sleep 10' ]
  restartPolicy: Never
  terminationGracePeriodSeconds: 0
---
# Source: python-time-service/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "python-time-service-test-connection"
  labels:
    helm.sh/chart: python-time-service-0.1.0
    app.kubernetes.io/name: python-time-service
    app.kubernetes.io/instance: python-time-service
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['python-time-service:8000']
  restartPolicy: Never
MANIFEST:
---
# Source: python-time-service/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: python-time-service
  labels:
    helm.sh/chart: python-time-service-0.1.0
    app.kubernetes.io/name: python-time-service
    app.kubernetes.io/instance: python-time-service
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: python-time-service/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: python-time-service-config
data:
  config.json: |-
    {
        "app_name": "Python Time Service",
        "author": "nai1ka",
        "course": "devops",
        "file_path": "/app/data/visits.txt"
    }
---
# Source: python-time-service/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: python-time-service
  labels:
    helm.sh/chart: python-time-service-0.1.0
    app.kubernetes.io/name: python-time-service
    app.kubernetes.io/instance: python-time-service
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: python-time-service
    app.kubernetes.io/instance: python-time-service
---
# Source: python-time-service/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: python-time-service
  labels:
    helm.sh/chart: python-time-service-0.1.0
    app.kubernetes.io/name: python-time-service
    app.kubernetes.io/instance: python-time-service
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: python-time-service
  podManagementPolicy: Parallel
  replicas: 4
  volumeClaimTemplates:
  - metadata:
      name: python-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
  selector:
    matchLabels:
      app.kubernetes.io/name: python-time-service
      app.kubernetes.io/instance: python-time-service
  template:
    metadata:
      annotations:
        name: ""
      labels:
        helm.sh/chart: python-time-service-0.1.0
        app.kubernetes.io/name: python-time-service
        app.kubernetes.io/instance: python-time-service
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: python-time-service
      containers:
        - name: python-time-service
          image: "nai1ka/time-service:latest"
          imagePullPolicy: Always
          envFrom:
            - configMapRef:
                name: python-time-service-config
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            limits:
              cpu: 500m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 128Mi
          volumeMounts:
            - mountPath: /app/data
              name: python-data

NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=python-time-service,app.kubernetes.io/instance=python-time-service" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
```

## Task 2: StatefulSet Exploration and Optimization

### 1. Research and Documentation

```bash
❱❱❱ kubectl get po,sts,svc,pvc                           
NAME                        READY   STATUS    RESTARTS   AGE
pod/python-time-service-0   1/1     Running   0          3m18s
pod/python-time-service-1   1/1     Running   0          3m18s
pod/python-time-service-2   1/1     Running   0          3m18s
pod/python-time-service-3   1/1     Running   0          3m18s


NAME                                   READY   AGE
statefulset.apps/python-time-service   4/4     3m18s

NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes            ClusterIP   10.96.0.1       <none>        443/TCP    17d
service/python-time-service   ClusterIP   10.100.153.74   <none>        8000/TCP   3m19s

NAME                                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/python-data-python-time-service-0   Bound    pvc-a9be3e6d-1a61-49d6-85b0-385538621ee8   1Gi        RWO            standard       <unset>                 3m19s
persistentvolumeclaim/python-data-python-time-service-1   Bound    pvc-dc653d9c-db38-4828-b48c-f377bedb643c   1Gi        RWO            standard       <unset>                 3m19s
persistentvolumeclaim/python-data-python-time-service-2   Bound    pvc-52447d7f-fe44-434a-972c-b8e6ee878f79   1Gi        RWO            standard       <unset>                 3m19s
persistentvolumeclaim/python-data-python-time-service-3   Bound    pvc-2ec715a7-ded3-4205-9774-c1e29aadab7f   1Gi        RWO            standard       <unset>                 3m19s
```

```bash
❱❱❱ minikube service python-time-service

❱❱❱ kubectl exec pod/python-time-service-0 -- cat /app/data/visits.txt
195                                               

❱❱❱ kubectl exec pod/python-time-service-1 -- cat /app/data/visits.txt
220                                                 

❱❱❱ kubectl exec pod/python-time-service-2 -- cat /app/data/visits.txt
205

❱❱❱ kubectl exec pod/python-time-service-3 -- cat /app/data/visits.txt
204                                           
```

Number of visits differs because the pods are not synchronized. Each pod has its own persistent volume and keeps its own visit count. When user enters the website, load balancer sends the request to one of the pods, and the visit count is being incremented only on this pods volume

### 2. Persistent Storage Validation

```bash
❱❱❱ kubectl delete pod python-time-service-0 
pod "python-time-service-0" deleted

❱❱❱ kubectl get pvc
NAME                                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
python-data-python-time-service-0   Bound    pvc-a9be3e6d-1a61-49d6-85b0-385538621ee8   1Gi        RWO            standard       <unset>                 71m
python-data-python-time-service-1   Bound    pvc-dc653d9c-db38-4828-b48c-f377bedb643c   1Gi        RWO            standard       <unset>                 71m
python-data-python-time-service-2   Bound    pvc-52447d7f-fe44-434a-972c-b8e6ee878f79   1Gi        RWO            standard       <unset>                 71m
python-data-python-time-service-3   Bound    pvc-2ec715a7-ded3-4205-9774-c1e29aadab7f   1Gi        RWO            standard       <unset>                 71m

❱❱❱ kubectl exec pod/python-time-service-0 -- cat /app/data/visits.txt
195                                        
```

### 3. Headless Service Access

```bash
 ❱❱❱ kubectl exec python-time-service-0 -- nslookup  python-time-service-1.python-time-service.default.svc.cluster.local
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   python-time-service-1.python-time-service.default.svc.cluster.local
Address: 10.244.1.90


❱❱❱ kubectl exec python-time-service-1 -- nslookup  python-time-service-0.python-time-service.default.svc.cluster.local
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   python-time-service-0.python-time-service.default.svc.cluster.local
Address: 10.244.1.88
```

### 4. Monitoring and Alerts

I have liveness probe defined in the `values.yaml` and then used in StatefulSet.

```yaml
livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http
```

```yaml
{{- with .Values.livenessProbe }}
    livenessProbe:
        {{- toYaml . | nindent 12 }}
{{- end }}
```

Liveness probes sends a request to the `/` path of the container and expects a response with status code 200. If the response is not 200, the container is restarted.

Liveness probes are very important for the StatefulSet to ensure that traffic is sent only to the healthy pods.
If the traffic is sent to the unhealthy pod, the data may be lost or corrupted leading to inconsistent state

### 5. Ordering Guarantee and Parallel Operations

Ordering guarantees are unnecessary for my apps, because each pod keeps its own visit count on its attached persistent volume. They don't depend on each other's state

To insturct the StatefulSet to perform operations in parallel, I added the following line to the StatefulSet spec:

```yaml
 podManagementPolicy: {{ .Values.podManagementPolicy }}
```

and then in the `values.yaml`:

```yaml
podManagementPolicy: Parallel
```

With parallel pod management enabled, pods can be added or removed without waiting for others, ensuring faster scaling and recovery

## Bonus Task

```bash
 ❱❱❱ kubectl get po,sts,svc,pvc             
NAME                        READY   STATUS             RESTARTS        AGE
pod/kotlin-time-service-0   1/1     Running            0               6m22s
pod/kotlin-time-service-1   1/1     Running            0               6m22s
pod/kotlin-time-service-2   1/1     Running            0               6m22s
pod/kotlin-time-service-3   1/1     Running            0               6m22s
pod/python-time-service-0   1/1     Running            0               3m16s
pod/python-time-service-1   1/1     Running            0               3m16s
pod/python-time-service-2   1/1     Running            0               3m16s
pod/python-time-service-3   1/1     Running            0               3m15s

NAME                                   READY   AGE
statefulset.apps/kotlin-time-service   4/4     6m24s
statefulset.apps/python-time-service   4/4     3m18s

NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kotlin-time-service   ClusterIP   10.99.213.247   <none>        8080/TCP   6m24s
service/kubernetes            ClusterIP   10.96.0.1       <none>        443/TCP    17d
service/python-time-service   ClusterIP   10.96.241.201   <none>        8000/TCP   3m19s

NAME                                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/kotlin-data-kotlin-time-service-0   Bound    pvc-5376a6a2-dec5-41c9-9c6d-3fdf6a6c9f38   1Gi        RWO            standard       <unset>                 6m24s
persistentvolumeclaim/kotlin-data-kotlin-time-service-1   Bound    pvc-c3b750e4-37fe-40de-9606-fae8d0c6f913   1Gi        RWO            standard       <unset>                 6m3s
persistentvolumeclaim/kotlin-data-kotlin-time-service-2   Bound    pvc-f118ca6e-ce2e-44ef-b022-62b015242841   1Gi        RWO            standard       <unset>                 6m3s
persistentvolumeclaim/kotlin-data-kotlin-time-service-3   Bound    pvc-ea084f3a-0d88-436e-bb36-f38bd9eda02e   1Gi        RWO            standard       <unset>                 6m2s
persistentvolumeclaim/python-data-python-time-service-0   Bound    pvc-d4cd7b2b-ce8f-48b4-bc69-93d74aa77ef4   1Gi        RWO            standard       <unset>                 136m
persistentvolumeclaim/python-data-python-time-service-1   Bound    pvc-0c97549c-f97c-4b4b-aa72-708d34ce7b5f   1Gi        RWO            standard       <unset>                 136m
persistentvolumeclaim/python-data-python-time-service-2   Bound    pvc-e93068cc-7935-4f7b-9e5b-5d59781b9dd6   1Gi        RWO            standard       <unset>                 136m
persistentvolumeclaim/python-data-python-time-service-3   Bound    pvc-0775e6fe-8554-4755-845b-6457a21595e0   1Gi        RWO            standard       <unset>                 136m
```

### Update strategies

I have added the following lines to the `statefulset.yaml` of both applications:

```yaml
updateStrategy:
     type: RollingUpdate
     rollingUpdate:
       partition: 1
```

#### OnDelete

- Pods are only update when manually deleted
- Use cases
  - When you want to control the update process manually
  - Environemnts where automatic updates are not allowed
  - When you want to ensure that only specific pods are updated at a given time

#### RollingUpdate

- Pods are automatically updated one by one
- the `partition` field can be used to control the number of pods that are updated at the
 same time (If a partition is specified, all Pods with an ordinal that is greater than or equal to the partition will be updated - from docs)
- Ensures that the application remains highly available during the update.
- If a new pod fails, the old one remains running, preventing downtime
- Use cases
  - When you dont want any owntime
  - When you want automatic updates without manual intervention
  - When you want to limit the number of pods being updated simultaneously to reduce risk
